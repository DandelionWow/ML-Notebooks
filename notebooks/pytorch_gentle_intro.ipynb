{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to PyTorch\n",
    "\n",
    "In our previous PyTorch [notebook](https://medium.com/dair-ai/pytorch-1-2-quickstart-with-google-colab-6690a30c38d), we learned about how to get started quickly with PyTorch using Google Colab. In this tutorial, we are going to take a step back and review some of the basic components of building a deep learning model using PyTorch. \n",
    "\n",
    "This will be a brief tutorial and will avoid using jargon and overcomplicated code. That said, this is perhaps the most basic of models you can build with PyTorch. \n",
    "\n",
    "If fact, it is so basic that it's ideal for those starting to learn about PyTorch and deep learning. So if you have a friend or colleague that wants to jump in, I highly encourage you to refer them to this tutorial as a starting point. Let's get started!\n",
    "\n",
    "\n",
    "**Author:** [Elvis Saravia](https://twitter.com/omarsar0)\n",
    "\n",
    "**Complete Code Walkthrough:** [Blog post](https://medium.com/dair-ai/pytorch-1-2-introduction-guide-f6fa9bb7597c)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "We need to import a few modules which will be useful to obtain the necessary functions that will help us to build our deep learning model. The main ones are `torch` and `torchvision`. They contain the majority of the functions that you need to get started with PyTorch. However, as this is a deep learning tutorial we will need `torch.nn`, `torch.nn.functional` and `torchvision.transforms` which all contain utility functions to build our model. We probably won't use all the modules listed below but they are the typical modules you will be importing when starting your deep learning projects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The usual imports\n",
    "import torch # 导包\n",
    "import torch.nn as nn # 导包\n",
    "import torch.nn.functional as F # 导包\n",
    "import torchvision # 导包\n",
    "import torchvision.transforms as transforms # 导包\n",
    "\n",
    "## for printing image\n",
    "import matplotlib.pyplot as plt # 导包\n",
    "import numpy as np # 导包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "Let's get right into it! As with any machine learning project, you need to load your dataset. We are using the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), which is the Hello World of datasets in the machine learning world. \n",
    "\n",
    "The data consists of number images that are of size `28 X 28`. We will discuss the images shortly, but our plan is to load data into batches of size `32`, similar to the figure below.\n",
    "\n",
    "\n",
    "![alt text](https://drive.google.com/uc?export=view&id=19AC_WpscyXkrK_o4PaFFGpt_jG0aJm_f)\n",
    "\n",
    "\n",
    "Here are the complete steps we are performing when importing our data:\n",
    "- We will import and tranform the data into tensors using the `transforms` module\n",
    "- We will use `DataLoader` to build convenient data loaders, which makes it easy to efficiently feed data in batches to deep learning models. We will get to the topic of batches in a bit but for now just think of them as subsets of your data. \n",
    "- As hinted above, we will also create batches of the data by setting the `batch` parameter inside the data loader. Notice we use batches of `32` in this tutorial but you can change it to `64` if you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter denoting the batch size\n",
    "BATCH_SIZE = 32 # 设置batch_size大小为32，意思是32张图片为1批\n",
    "\n",
    "## transformations\n",
    "transform = transforms.Compose( # Compose，一般用Compose把多个步骤整合到一起\n",
    "    [transforms.ToTensor()]) # ToTensor，将图像（PIL Image）转为张量，PIL(Python Image Library)是python的第三方图像处理库\n",
    "\n",
    "## download and load training dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform) # 下载训练集\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, # 负责对数据进行洗牌和构建批处理。\n",
    "                                          shuffle=True, num_workers=2) # 训练集加载器，可以看到训练集的shape是[60000, 28, 28]，每张图片是28*28，共60000张\n",
    "\n",
    "## download and load testing dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform) # 下载测试集\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2) # 测试集加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect what the trainset and testset objects contain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the trainset and testset\n",
    "print(trainset) # 打印\n",
    "print(testset) # 打印"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a beginner's tutorial so I will break down things a bit here:\n",
    "- `BATCH_SIZE` is a parameter that denotes the batch size we will use for our model\n",
    "- `transform` holds code for whatever transformations you will apply to your data. I will show you an example below to demonstrate exactly what it does to shed more light into its use\n",
    "- `trainset` and `testset` contain the actual dataset object. Notice I use `train=True` to specify that this corresponds to the training dataset, and I use `train=False` to specify that this is the remainder of the dataset which we call the testset. From the portion I printed above you can see that the split of the data was 85% (60000) / 15% (10000), corresponding to the portions of samples for training set and testing set, respectively. \n",
    "- `trainloader` is what holds the data loader object which takes care of shuffling the data and constructing the batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at that `transforms.Compose(...)` function and see what it does. We will use a randomized image to demonstrate its use. Let's generate an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn(1, 96, 96) # 创建一个形状为1单通道*96*96的张量\n",
    "# print(tensor.shape)\n",
    "# mode的取值：https://blog.csdn.net/qq_36430012/article/details/114289398\n",
    "image = transforms.ToPILImage(mode='L')(tensor) # ToPILImage，将张量转为图片（C通道数*H高度*W宽度）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And** let's render it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image) # 展示图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have our image sample. And now let's apply some dummy transformation to it. We are going to rotate the image by `45` degrees. The transformation below takes care of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dummy transformation\n",
    "\"\"\" RandomRotation 对图片旋转随机的角度\n",
    "    args: \n",
    "        degrees, 旋转角度。当degrees为a时, 在区间(-a,a)之间随机选择旋转角度. 当degrees为(a,b)时，在区间(a,b)之间随机选择旋转角度.\n",
    "\"\"\"\n",
    "dummy_transform = transforms.Compose(\n",
    "    [transforms.RandomRotation(45)]) # RandomRotation，对图片旋转随机的角度\n",
    "\n",
    "dummy_result = dummy_transform(image) # 经过旋转后的新图像\n",
    "\n",
    "plt.imshow(dummy_result) # 展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice you can put the transformations within `transforms.Compose(...)`. You can use the built in transformations offered by PyTorch or you can build your own and compose as you wish. In fact, you can place as many transformation as you wish in there. Let's try another composition of transformations: rotate + vertical flip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dummy transform \n",
    "\"\"\" RandomVerticalFlip 根据概率对图片进行垂直（上下）翻转，每次根据概率来决定是否执行翻转\n",
    "    args: \n",
    "        p: 反转概率, 默认0.5\n",
    "\"\"\"\n",
    "dummy2_transform = transforms.Compose(\n",
    "    [transforms.RandomRotation(45), transforms.RandomVerticalFlip()]) # 两次变换，先随机在(-45, 45)旋转，再翻转\n",
    "\n",
    "dummy2_result = dummy2_transform(image) # 新图像\n",
    "\n",
    "plt.imshow(dummy2_result) # 展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty cool right! Keep trying other transform methods. On the topic of exploring our data further, let's take a look at our images dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "As a practioner and researcher, I am always spend a bit of time and effort exploring and understanding my datasets. It's fun and this is a good practise to ensure that everything is in order.\n",
    "\n",
    "Let's check what the train and test dataset contain. I will use matplotlib to print out some of the images from our dataset. With a bit of numpy I can convert images into numpy and print them out. Below I print out an entire batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to show an image\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy() # torch张量转为numpy\n",
    "    # transpose，就是调换数组中的索引值，比如一个三维数组形状是(x, y, z)，索引值是(0, 1, 2)，经过transpose(npimg, (1, 2, 0))，变为(y, z, x)，索引值是(1, 2, 0)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # 通道数需要在最后\n",
    "\n",
    "## get some random training images\n",
    "dataiter = iter(trainloader) # 迭代器\n",
    "images, labels = next(dataiter) # torch版本问题，dataiter.next()报错，改为next(dataiter)\n",
    "## show images\n",
    "imshow(torchvision.utils.make_grid(images)) # make_grid，将多张图片组合成一张图片，默认每一行8张，间距2。images中有32张图片，每张的shape为[1, 28, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions of our batches are as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in trainloader:\n",
    "    print(\"Image batch dimensions:\", images.shape) # 打印图片形状\n",
    "    print(\"Image label dimensions:\", labels.shape) # 打印标记形状\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "Now it's time to build the deep learning model that will be used to perform the image classification. We will keeps things simple and stack a few dense layers and a dropout layer to train our model.\n",
    "\n",
    "Let's discuss a bit about the model:\n",
    "\n",
    "- First of all the following structure involving a `class` is standard code that's used to build the neural network model in PyTorch:\n",
    "\n",
    "```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # layers go here\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # computations go here\n",
    "```\n",
    "- The layers are defined inside `def __init__()`. `super(...).__init__()` is just there to stick things together. For our model, we stack a hidden layer (`self.d1`) followed by a dropout layer (`self.dropout`), which is then followed by an output layer (`self.d2`). \n",
    "- `nn.Linear(...)` defines the dense layer and it requires the `in` and `out` dimensions, which corresponds to the size of the input feature and output feature of that layer, respectively. \n",
    "- `nn.Dropout(...)` is used to define a dropout layer. Dropout is an approach in deep learning that helps a model to avoid overfitting. This means that dropout acts as a regularization technique that helps the model to not overfit on the images it has seen while training. We want this because we need a model that generalizes well to unseen examples -- in our case, the testing dataset. Dropout randomly zeroes some of the units of the neural network layer with probability of `p=0.2`. Read more about the dropout layer [here](https://pytorch.org/docs/stable/nn.html#dropout). \n",
    "- The entry point of the model, i.e. where the data enters, is placed under the `forward(...)` function. Typically, we also place other transformations we perform on the data while training inside this function. \n",
    "- In the `forward()` function we are performing a series of computations on the input data\n",
    "    - we flatten the images first, converting it from 2D (`28 X 28`) to 1D (`1 X 784`).\n",
    "    - then we feed the batches of those 1D images into the first hidden layer\n",
    "    - the output of that hidden layer is then applied a [non-linear activate function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) called `ReLU`. It's not so important to know what `F.relu()` does at the moment, but the effect that it achieves is that it allows faster and more effective training of neural architectures on large datasets\n",
    "    - as explained above, the dropout also helps the model to train more efficiently by avoiding overfitting on the training data\n",
    "    - we then feed the output of that dropout layer into the output layer (`d2`)\n",
    "    - the result of that is then fed to the [softmax function](https://en.wikipedia.org/wiki/Softmax_function), which converts or normalized the output into a probability distribution which helps with outputting proper predictions values that are used to calculate the accuracy of the model; this will the final output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.d1 = nn.Linear(28 * 28, 128) # 第一层，隐藏层\n",
    "        self.dropout = nn.Dropout(p=0.2) # 正则，防止过拟合\n",
    "        self.d2 = nn.Linear(128, 10) # 输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1) # 将图像从1*28*28转为784长度的一维数组\n",
    "        x = self.d1(x) # 进入隐藏层\n",
    "        x = F.relu(x) # 非线性激活函数relu\n",
    "        x = self.dropout(x) # 加入正则\n",
    "        logits = self.d2(x) # 输出层\n",
    "        out = F.softmax(logits, dim=1) # softmax，转换或规范化输出到一个概率分布\n",
    "        return out # 返回结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, the following is a diagram of the model we have built. Just keep in mind that the hidden layer is much bigger as shown in the diagram but due to space constraint, the diagram is just an approximation to the actual model. \n",
    "\n",
    "![alt text](https://drive.google.com/uc?export=view&id=1NuFflDPOW_hIAHTH2pXZAEhSINygPlnB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I have done in my previous tutorials, I always encourage to test the model with 1 batch to ensure that the output dimensions are what we expect. Notice how we are iterating over the dataloader which conveniently stores the `images` and `labels` pairs. `out` contains the output of the model, which are the logits applied a `softmax` layer which helps with prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test the model with 1 batch\n",
    "model = MyModel() # 实例化模型\n",
    "for images, labels in trainloader: # 循环训练集\n",
    "    print(\"batch size:\", images.shape) # 打印一个batch的形状\n",
    "    out = model(images) # 训练一个batch试试\n",
    "    print(out.shape) # 输出形状\n",
    "    break # 结束一个batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that we get back the batches with 10 output values associate with it. These are used to compute the performance of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Now we are ready to train the model but before that we are going to setup a loss function, an optimizer and a function to compute accuracy of the model. \n",
    "\n",
    "- The `learning_rate` is the rate at which the model will try to optimize its weights, which is just another parameter for the model. \n",
    "- `num_epochs` is the number of training steps. \n",
    "- `device` determines what hardware we will use to train the model. If a `gpu` is present, then that will be used, otherwise it defaults to the `cpu`.\n",
    "- `model` is just the model instance.\n",
    "- `model.to(device)` is in charge of setting the actaull device that will be used for training the model\n",
    "- `criterion` is just the metric that's used to compute the loss of the model while it forward and backward trains to optimize its weights. \n",
    "- `optimizer` is the optimization technique used to modify the weights in the backward propagation. Notice that it requires the `learning_rate` and the model parameters which are part of the calculation to optimize weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 # 学习率 \n",
    "num_epochs = 5 # 同一个数据集训练几次\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 如果有GPU(cuda)就用，否则用cpu\n",
    "model = MyModel() # 实例化模型\n",
    "model = model.to(device) # 指定在哪个设备上训练model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # 设置优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility function below helps to calculate the accuracy of the model. For now, it's not important to understand how it's calculated but basically it compares the outputs of the model (predictions) with the actual target values (i.e., the labels of the dataset), and tries to compute the average of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utility function to compute accuracy\n",
    "def get_accuracy(output, target, batch_size):\n",
    "    ''' Obtain accuracy for training round '''\n",
    "    corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Now it's time to train the model. The code portion that follows can be descrive in the following steps:\n",
    "\n",
    "- The first thing in training a neural network model is defining the training loop, which is achieved by:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    ...\n",
    "```\n",
    "\n",
    "- We define two variables, `training_running_loss` and `train_acc` that will help us to monitor the running accuracy and loss of the modes while it trains over the different batches.\n",
    "- `model.train()` explicitly indicates that we are ready to start training. \n",
    "- Notice how we are iterating over the dataloader, which conveniently gives us the batches in image-label pairs. \n",
    "- That second `for` loop means that for every training step we will iterate over all the batches and train the model over them.\n",
    "- We feed the model the images via `model(images)` and the output are the predictions of the model. \n",
    "- The predictions together with the target labels are used to compute the loss using the loss function we defined earlier.\n",
    "- Before we update our weights for the next round of training, we perform the following steps:\n",
    "    - we use the optimizer object to reset all the gradients for the variables it will update. This is a safe step and it doesn't overwrites the gradients the model accumulates while training (those are stored in a buffer [link text](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-optim) via the `loss.backward() call)\n",
    "    - `loss.backward()` simply computes the gradient of the loss w.r.t to the model parameters\n",
    "    - `optimizer.step()` then ensures that the model parameters are updated\n",
    "\n",
    "- Then we gather and accumulate the loss and accuracy, which is what we will use to tell us if the model is learning properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the model\n",
    "for epoch in range(num_epochs): # 循环5次\n",
    "    train_running_loss = 0.0 # 损失初始化\n",
    "    train_acc = 0.0 # 精确度初始化\n",
    "\n",
    "    ## commence training\n",
    "    model = model.train() # 开始训练\n",
    "\n",
    "    ## training step\n",
    "    for i, (images, labels) in enumerate(trainloader): # 每次循环一个batch\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        ## forward + backprop + loss\n",
    "        predictions = model(images) # 预测\n",
    "        loss = criterion(predictions, labels) # 计算loss\n",
    "        optimizer.zero_grad() # 清空梯度\n",
    "        loss.backward() # 开启非叶计算梯度\n",
    "\n",
    "        ## update model params\n",
    "        optimizer.step() # 更新模型参数\n",
    "\n",
    "        train_running_loss += loss.detach().item() # 累加loss\n",
    "        train_acc += get_accuracy(predictions, labels, BATCH_SIZE) # 累加精度\n",
    "    \n",
    "    model.eval() # 模型评估\n",
    "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n",
    "          %(epoch, train_running_loss / i, train_acc / i))  # 打印每个epoch的平均loss和acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the training steps are over, we can clearly see that the loss keeps decreasing while the training accuracy of the model keeps rising, which is a good sign that the model is effectively learning to classify images.\n",
    "\n",
    "We can verify that by computing the accuracy on the testing dataset to see how well the model performs on the image classificaiton task. As you can see below, our basic CNN model is performing very well on the MNIST classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = 0.0 # 测试精度初始化\n",
    "for i, (images, labels) in enumerate(testloader, 0): # 遍历测试集\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images) # 预测\n",
    "    test_acc += get_accuracy(outputs, labels, BATCH_SIZE) # 精度累加\n",
    "        \n",
    "print('Test Accuracy: %.2f'%( test_acc/i)) # 测试集精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Words\n",
    "\n",
    "Congratulation! You have made it to the end of this tutorial. This is a really long tutorial that aims to give an very basic introduction to the fundamentals of image classification using neural networks and PyTorch.\n",
    "\n",
    "*This tutorial was heavily inspired by this [TensorFlow tutorial.](https://www.tensorflow.org/beta/tutorials/quickstart/beginner) We thank the authors of the corresponding reference for their valuable work.*\n",
    "\n",
    "## References\n",
    "- [PyTorch 1.2 Quickstart with Google Colab](https://medium.com/dair-ai/pytorch-1-2-quickstart-with-google-colab-6690a30c38d)\n",
    "- [Get started with TensorFlow 2.0 for beginners](https://www.tensorflow.org/beta/tutorials/quickstart/beginner)\n",
    "- [PyTorch Data Loading Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
    "-[ Neural Networks with PyTorch](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "154abf72fb8cc0db1aa0e7366557ff891bff86d6d75b7e5f2e68a066d591bfd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
